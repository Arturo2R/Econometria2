[["index.html", "Econometría 2 Prefacio", " Econometría 2 Andrés Vargas 2022-02-09 Prefacio Notas de clase de la asignatura Econometría 2, del programa de Economía de la Universidad del Norte. Están bajo permanente revisión y actualización. Comentarios o sugerencias a andresmv@uninorte.edu.co "],["sobre-el-autor.html", "Sobre el autor Andrés Vargas", " Sobre el autor Andrés Vargas Economista, profesor adscrito al departamento de economía de la Universidad del Norte, Colombia. "],["introducción.html", "Introducción", " Introducción Este curso se divide en dos partes. En la primera se aborda la idea de inferencia causal en estructuras de datos transversales y longitudinales. A partir de las propiedades del estimador de Mínimos Cuadrados Ordinarios se identifican las circunstancias bajo las cuales se le puede dar una interpretación causal a los parámetros estimados. Cuando no es posible, se examinan métodos alternativos de estimación. La segunda parte del curso introduce a las series de tiempo. Tomando como punto de partida los conceptos de dependencia débil y estacionariedad se estudian modelos dinámicos univariados y multivariados. "],["el-modelo-de-regresión-lineal-y-el-estimador-mco.html", "Capítulo1 El modelo de regresión lineal y el estimador MCO 1.1 La mecánica del estimador MCO 1.2 Estimador 1.3 Estimador MCO", " Capítulo1 El modelo de regresión lineal y el estimador MCO La tabla muestra los resultados para la estimación del siguiente modelo \\[\\begin{equation} sales=\\beta_0+\\beta_1price+\\beta_2advert+e \\end{equation}\\] Donde sales son los ingresos mensuales de una compañía, en miles de dólares, price es un índice de precios de todos los productos vendidos, y advert es el gasto mensual en publicidad, también en miles de dólares. Table 1.1: Modelo básico de regresión lineal Coefficient Std. Error t-value p-value (Intercept) 118.914 6.352 18.722 0.000 price -7.908 1.096 -7.215 0.000 advert 1.863 0.683 2.726 0.008 Interprete los resultados Explique la magnitud del coeficiente estimado Explique cada una de las columnas ¿Son las variables significativas? Si la respuesta es afirmativa, ¿esto que quiere decir? Desde el punto de vista práctico, los números que mas nos interesan en la tabla son la estimación puntual del coeficiente y el error estándar. Con el primero examinamos la dirección y magnitud del efecto de \\(x\\) sobre \\(y\\), con el segundo tenemos una idea de la precisión de la estimación y nos permite indagar si el valor estimado del parámetro es diferente de cero, en términos estadísticos. Veamos esto en detalle 1.1 La mecánica del estimador MCO Suponga que usted tiene los siguientes datos La regresión lineal no es más que buscar la línea que mejor se ajuste a estos datos. La forma más simple de estimar los parámetros es usando el estimador de Mínimos Cuadrados Ordinarios, MCO. La idea es la siguiente. Planteamos una relación lineal entre \\(y\\) y \\(x\\) \\[\\begin{equation} y=\\alpha+\\beta x+e \\end{equation}\\] En esta ecuación \\(\\alpha\\) es el intercepto y \\(\\beta\\) la pendiente. El término \\(e\\) lo llamamos el error. Note que el error recoge la diferencia entre lo que observamos de \\(y\\) y lo que \\(x\\) predice que será \\(y\\). Si \\(\\alpha=100\\) y \\(\\beta=10\\) entonces podemos decir que si \\(x=10\\) entonces \\(y=200\\). Sin embargo, al observar la gráfica se dará cuenta que hay muchas puntos donde \\(x=10\\) pero \\(y\\neq200\\). Esa diferencia es \\(e\\) ¿Por qué podría darse esa diferencia entre lo predicho y lo observado? Varias razones Hay otras variables, ej. \\(z\\), que pueden afectar el comportamiento de \\(y\\), y que no hemos incluido Variabilidad aleatoria. Digamos que \\(\\hat{y}\\) es la línea de regresión, y esta es igual a \\[\\begin{equation} \\hat{y}=\\hat{\\alpha}+\\hat{\\beta}x \\end{equation}\\] La distancia entre cada punto y la línea de regresión es \\[\\begin{equation} \\hat{e}_i=y_i-\\hat{y}_i=y_i-\\hat{\\alpha}-\\hat{\\beta}x_i \\end{equation}\\] Los parámetros que producen la mejor línea son aquellos que minimizan la suma de los residuales al cuadrado \\[\\begin{equation} SSE=\\sum_i\\hat{e}_i^2 \\end{equation}\\] Lo anterior quiere decir, que aún cuando hay muchas líneas que recogen la relación positiva entre nuestras variables, hay una que es la mejor de todas El estimador MCO, puede entenderse como un algoritmo para encontrar la mejor línea entre todas las posibles. Donde mejor significa la que minimiza la suma de residuales al cuadrado. En términos más simples, la línea que se equivoca menos. 1.2 Estimador Decimos que estimamos los parámetros \\(\\boldsymbol{\\beta}\\) porque partimos de la idea de que nuestras variables \\(y\\) y \\(x\\) son variables que representan a alguna población. Sin embargo, nosotros tenemos una muestra particular. Es decir, nuestros datos son una única muestra de muchas muestras posibles que pudimos haber obtenido de la misma población. Por ejemplo, supongamos que nuestra población de interés son los estudiantes de economía de la Universidad del Norte, \\(N=180\\). Sea \\(PGA\\) el promedio académico de cada estudiante. Queremos calcular el promedio de \\(PGA\\) para la población de estudiantes del programa de economía. El parámetro poblacional es el PGA promedio para todos los estudiantes, \\(\\mu=E(PGA)\\). Por alguna razón, no es posible obtener ese dato para todos los estudiantes, sino que tenemos a nuestra disposición una muestra aleatoria de 50 estudiantes, \\(n_1=50\\). Con esta muestra calculamos (estimamos) un PGA promedio de 3.6. Llamemos a esta estimación \\(\\hat{\\mu}_1=\\bar{PGA}_1=\\dfrac{1}{n_1}\\sum_{i=1}^{n_1}PGA_i\\) Imagine que hubiésemos obtenido una muestra diferente, \\(n_2=50\\), y con esta muestra calculamos un \\(\\hat{\\mu}_2=3.5\\). Debe ser claro que siempre que estimamos, el número que obtenemos va a depender de la muestra que tengamos. La variabilidad muestral es inevitable, así que lo importante es tenerla en cuenta para poder evaluar que tan bueno es nuestro proceso de estimación. Así, mientras \\(\\mu\\) es un parámetro poblacional \\(\\hat{\\mu}\\) es un estimador de dicho parámetro poblacional. El parámetro poblacional es una característica no aleatoria de la población, mientras que el estimador si es aleatorio. \\(\\mu\\) está fijo y \\(\\hat{\\mu}\\) varía entre muestras. Veamos Supongamos que podemos tomar 100 muestras diferentes de tamaño 50. Para cada muestra calculamos el PGA promedio, \\(\\mu_i\\) donde \\(i=1,...100\\). Veamos como se distribuye el PGA promedio estimado en cada muestra. library(msm) set.seed(12345) #para iniciar la obtención de números aleatorios pga&lt;-rtnorm(180,mean=3.6,sd=0.4,lower=1,upper=5) nrsim&lt;-100 vpga&lt;-numeric(nrsim) for (i in 1:nrsim){ pgasi&lt;-sample(pga,50,replace=TRUE,prob=NULL) vpga[i]&lt;-mean(pgasi) } library(ggplot2) vpgadf&lt;-data.frame(vpga) ggplot(vpgadf,aes(x=vpga))+geom_density(fill=&quot;lightblue&quot;,alpha=0.4)+geom_vline(xintercept=mean(vpga),linetype=4)+theme_minimal()+labs(title=&quot;Distribución del promedio de PGA&quot;,subtitle=&quot;100 muestras diferentes&quot;,x=&quot;PGA promedio&quot;) El promedio de las estimaciones en las 100 muestras, \\(\\dfrac{1}{100}\\sum_{i=1}^{100}\\hat{\\mu}_i\\), es round(mean(vpga),digits=2) ## [1] 3.65 Este valor lo comparamos con el valor del parámetro poblacional, que en este caso es round(mean(pga),digits=2) ## [1] 3.65 Así, podemos decir que nuestro procedimiento para estimar el promedio de PGA a partir de una muestra es bueno, pues en promedio nos permitirá obtener el valor verdadero. En otras palabras \\[\\begin{equation} \\dfrac{1}{100}\\sum_{i=1}^{100}\\hat{\\mu}_i=\\mu \\end{equation}\\] 1.3 Estimador MCO Ya sabemos que el estimador MCO consiste en obtener los parámetros que minimizan la suma de los errores al cuadrado. El método de mínimos cuadrados consiste en obtener los valores de los parámetros \\(\\boldsymbol{\\beta}\\) que minimizan \\[SSE(\\beta)=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_{i1}-\\beta_2x_{i2}-...-\\beta_kx_{ik})^2\\] Si llamamos \\(\\boldsymbol{\\hat{\\beta}}\\) a los parámetros estimados con los datos que tenemos, entonces sabemos que la formula para computarlos es, en términos matriciales1 \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}=\\mathbf{(X&#39;X)^{-1}X&#39;Y} \\end{equation}\\] Donde \\(\\mathbf{X}\\) y \\(\\mathbf{Y}\\) son las matrices de datos, de dimensiones \\(nxk\\) y \\(nx1\\), respectivamente. Para poner las cosas simples, supongamos que tenemos el siguiente modelo \\[\\begin{equation} y=\\beta_0+\\beta_1x_1+\\beta_2x_2+e \\end{equation}\\] De la sección anterior sabemos que los estimadores (\\(\\hat{\\beta_0},\\hat{\\beta_1},\\hat{\\beta_2}\\)) son variables aleatorias. Toman diferentes valores en diferentes muestras, y sus valores son desconocidos hasta que la muestra es obtenida y sus valores computados. La distribución muestral del estimador MCO describe como varían estos estimadores sobre todas las muestras posibles. Las propiedades muestrales del estimador hacen referencia a la media y la varianza de de dicha distribución. Si la media de la distribución es igual al valor del parámetro poblacional, entonces decimos que el estimador es insesgado. En nuestro ejemplo de PGA esto es lo mismo que decir \\(E(\\hat{\\mu})=\\mu\\). La varianza de la distribución nos dice que tanto cambia el estimador entre muestras. Entre más cambie entre muestras, menos confiable es el estimador, menos preciso. 1.3.1 Simulación Simulemos un proceso y veamos estas ideas de forma precisa. Digamos que la ecuación poblacional es \\[\\begin{equation} Y=5+2.5X_1+3X_2+e \\end{equation}\\] y que nuestra población es \\(N=10,000\\). Como antes, supongamos que no conocemos el valor de los parámetros y los estimamos a partir de alguna muestra. Obtenemos muestras de tamaño \\(n=100\\), aproximadamente. Lo que queremos ver es si nuestro procedimiento nos permite obtener los parámetros verdaderos. Simularemos \\(1000\\) muestras library(mvtnorm) N&lt;-10000 coefs&lt;-cbind(&quot;hat_beta_1&quot; = numeric(1000), &quot;hat_beta_2&quot; = numeric(1000)) #Vector que guardará los coeficientes set.seed(1) # permite reproducir los resultados X &lt;- rmvnorm(N, c(50, 100), sigma = cbind(c(10, 2.5), c(2.5, 10))) # generamos X1 y X2 e &lt;- rnorm(N, sd = 5) Y &lt;- 5 + 2.5 * X[, 1] + 3 * X[, 2] + e xdf&lt;-data.frame(X,Y) nrsim&lt;-1000 #obtenemos 1000 muestras de nuestra población for (i in 1:nrsim){ dfs&lt;-sample(c(TRUE,FALSE),nrow(xdf),replace=TRUE,prob=c(0.01,0.99)) #muestra aleatoria, n aprox 100 dfs&lt;-xdf[dfs,] ols&lt;-lm(Y~X1+X2,data=dfs) #estimamos para cada muestra generada coefs[i,]&lt;-coef(ols)[-1] # el valor estimado de b1 y b2 en cada muestra se lleva al vector coefs } coefs.df&lt;-data.frame(coefs) Calculemos ahora el promedio de los valores estimados \\(\\hat{\\beta_1}\\) y \\(\\hat{\\beta_2}\\) library(dplyr) msd&lt;-coefs.df%&gt;%summarise(b1m=mean(coefs.df$hat_beta_1), b2m=mean(coefs.df$hat_beta_2), b1sd=sd(coefs.df$hat_beta_1), b2sd=sd(coefs.df$hat_beta_1) ) kable(msd, caption=&quot;Promedio y desviación estándar de los estimadores&quot;, col.names=c(&quot;Media b1&quot;, &quot;Media b2&quot;, &quot;SD b1&quot;, &quot;SD b2&quot;), align=&quot;c&quot;, digits=2) Table 1.2: Promedio y desviación estándar de los estimadores Media b1 Media b2 SD b1 SD b2 2.5 2.99 0.17 0.17 Veamos la distribución del estimador \\(\\hat{\\beta}_1\\) ggplot(coefs.df,aes(x=hat_beta_1))+geom_density(fill=&quot;lightblue&quot;,alpha=0.4)+ geom_vline(xintercept=mean(coefs.df$hat_beta_1),linetype=4)+theme_minimal()+ labs(title=&quot;Distribución del promedio del estimador&quot;,subtitle=&quot;1000 muestras diferentes&quot;,x=expression(beta[1]*estimado)) Con cada muestra se estima un valor diferente del parámetro de interés, pero si promediamos todas las estimaciones encontramos que \\(\\dfrac{1}{1000}\\sum_{i=1}^{1000}\\hat{\\beta}_{1i}=2.5\\). Este es exactamente el valor del parámetro poblacional. Podemos decir entonces que nuestro estimador es insesgado. Ejercicio Replique la actividad anterior, pero ahora varíe el tamaño de muestra: i) \\(n\\approx50\\); ii) \\(n\\approx1000\\). Compare la media y la desviación estándar del estimador ¿Qué puede concluir? En el siguiente capítulo lo elaboramos en detalle "],["estimador-mco-propiedades.html", "Capítulo2 Estimador MCO: Propiedades 2.1 El valor esperado del estimador MCO 2.2 La varianza del estimador en datos de corte transversal 2.3 Error estándar e inferencia", " Capítulo2 Estimador MCO: Propiedades Nuestro punto de partida es el modelo poblacional \\[\\begin{equation} \\tag{1} y=\\beta_1+\\beta_2x_2+\\beta_3x_3+...+\\beta_kx_k+u \\end{equation}\\] Donde donde las variables \\(y,x_2,...,x_k\\) son aleatorias y observables, y \\(u\\) es un error no observable. Los parámetros \\(\\beta_1,\\beta_2,...,\\beta_k\\) son los que queremos estimar. El error \\(u\\) recoge perturbaciones aleatorias, y también todo aquello que es importante para explicar \\(y\\) pero que no hemos incluido explícitamente en el modelo, es decir variables omitidas. La idea de población no hace referencia, necesariamente, a una población física en el mundo real. Significa que si tenemos una observación para el individuo i, \\((y_i,x_i)\\), esta la consideramos como la realización de una función de probabilidad conjunta \\(F(y,x)\\). Nosotros no conocemos \\(F\\), y el propósito de la inferencia es aprender sus características a partir de una muestra, es decir del conjunto particular de datos que tenemos. Lo anterior significa que a partir de nuestros datos estimamos los valores de \\(\\boldsymbol{\\beta}\\), y a estos los llamamos \\(\\hat{\\boldsymbol{\\beta}}\\) El estimador MCO consiste en estimar dichos parámetros a partir de encontrar el valor de ellos tales que se minimiza la diferencia al cuadrado entre el valor observado y el valor predicho, con una muestra particular de datos. Esto quiere decir que son aquellos que minimizan la expresión \\[\\begin{equation} \\tag{2} \\sum_i^n(y_i-\\hat{\\beta_1}-\\hat{\\beta_2}x_{i2}-...-\\hat{\\beta_k}x_{ik})^2 \\end{equation}\\] Donde \\(i=1,...,n\\) identifica cada observación en la muestra. Al tomar las condiciones del primer orden obtenemos \\[\\begin{align} \\tag{3} \\sum_i^n(y_i-\\hat{\\beta_1}-\\hat{\\beta_2}x_{i2}-...-\\hat{\\beta_k}x_{ik})&amp;=0\\\\ \\sum_i^nx_{i2}(y_i-\\hat{\\beta_1}-\\hat{\\beta_2}x_{i2}-...-\\hat{\\beta_k}x_{ik})&amp;=0\\\\ .&amp;\\\\ .&amp;\\\\ .&amp;\\\\ \\sum_i^nx_{ik}(y_i-\\hat{\\beta_1}-\\hat{\\beta_2}x_{i2}-...-\\hat{\\beta_k}x_{ik})&amp;=0 \\end{align}\\] Fíjese que tenemos un sistema de \\(k\\) ecuaciones con \\(k\\) incognitas. En términos matriciales esto lo podemos escribir como \\[\\begin{equation} \\tag{4} \\mathbf{X&#39;}(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\hat{\\beta}})=0 \\end{equation}\\]. Donde \\(\\mathbf{X}\\) es \\(n\\times k\\) y recoje los datos de las variables independientes, mientras que \\(\\mathbf{y}\\) es \\(n\\times 1\\) y contiene los valores de la variable dependiente, y es la matriz de parámetros estimados, de dimensión \\(k\\times 1\\). La expresión anterior es equivalente a \\[\\begin{equation} \\tag{5} \\mathbf{(X&#39;X)}\\hat{\\boldsymbol{\\beta}}=\\mathbf{X&#39;y} \\end{equation}\\] Si la matriz \\(\\mathbf{(X&#39;X)}\\) es invertible entonces podemos premultiplicar a ambos lados por \\(\\mathbf{(X&#39;X)}^{-1}\\) y obtenemos \\[\\begin{equation} \\tag{6} \\hat{\\boldsymbol{\\beta}}=\\mathbf{(X&#39;X)}^{-1}\\mathbf{X&#39;y} \\end{equation}\\] La matriz \\(\\mathbf{(X&#39;X)}\\) es invertible si no hay colinealidad perfecta entre las variables. Como el valor estimado de los parámetros se obtuvo de una muestra particular de datos entonces debemos tener en cuenta que pudimos haber observado una muestra diferente, con la cual el valor puntual estimado habría sido diferente. Nuestro objetivo es obtener las propiedades estadísticas del estimador 2.1 El valor esperado del estimador MCO S1 Modelo poblacional \\[\\begin{equation} y=\\beta_1+\\beta_2x_2+\\beta_3x_3+...+\\beta_kx_k+u \\end{equation}\\] S2 Tenemos una muestra aleatoria de tamaño \\(n\\), \\(\\{(x_{i1},x_{i2},...,x_{ik}):i=1,2,...,n\\}\\), es decir que las observaciones son independientes e identicamente distribuidas. Por ejemplo, el ingreso y nivel educativo del individuo \\(i\\) es independiente del individuo \\(j\\). S3 No hay colinealidad perfecta, y por lo tanto \\(\\mathbf{(X&#39;X)}\\) es invertible S4 \\(E(u|x_1,x_2,...,x_k)=0\\) El valor esperado condicional del error es cero. Es decir que el error no está relacionado con las variables independientes. Bajo estos supuestos, podemos mostrar que \\[\\begin{equation} \\tag{7} E(\\hat{\\boldsymbol{\\beta}}|\\mathbf{X})=0 \\end{equation}\\] Veamos. Primero tomemos el valor esperado condicional en la ecuación \\((6)\\) \\[\\begin{equation} \\tag{8} E(\\hat{\\boldsymbol{\\beta}}|\\mathbf{X})=\\mathbf{(X&#39;X)}^{-1}E(\\mathbf{X&#39;y}|\\mathbf{X}) \\end{equation}\\] Como \\(\\mathbf{y}=\\mathbf{X}\\boldsymbol{\\beta}+u\\), entonces \\[\\begin{equation} \\tag{9} E(\\hat{\\boldsymbol{\\beta}}|\\mathbf{X})=\\mathbf{(X&#39;X)}^{-1}\\mathbf{X&#39;X}\\boldsymbol{\\beta}+\\mathbf{(X&#39;X)}^{-1}\\mathbf{X&#39;}E(u|\\mathbf{X}) \\end{equation}\\] Luego, si se cumple S4 \\[\\begin{equation} \\tag{10} E(\\hat{\\boldsymbol{\\beta}}|\\mathbf{X})=\\boldsymbol{\\beta} \\end{equation}\\] Es decir que el estimador es insesgado. 2.2 La varianza del estimador en datos de corte transversal S5 Homocedasticidad, \\(Var(u|\\mathbf{X})=\\sigma^2\\). Es decir que la varianza condicional del error es la misma para todos los valores de las variables explicativas. Con S5 entonces podemos mostrar que \\[\\begin{equation} \\tag{11} Var(\\hat{\\boldsymbol{\\beta}}|\\mathbf{X})=\\sigma^2\\mathbf{(X&#39;X)}^{-1} \\end{equation}\\] Para entenderlo mejor, la varianza para un \\(\\beta_j\\) particular sería \\[\\begin{equation} Var(\\hat{\\beta_j}|\\mathbf{X})=\\dfrac{\\sigma^2}{\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2(1-R_j^2)} \\end{equation}\\] Donde \\(R_j^2\\) es el \\(R\\)-cuadrado de una regresión de \\(x_j\\) contra las demás independiente. Entre más correlacionada esté \\(x_j\\) con las demás variables, mayor será el \\(R\\)-cuadrado La varianza depende de tres cosas La varianza del error \\(\\sigma^2\\). Esto es una característica de la población. Si se agregan más variables esta podría reducirse. Sin embargo, si el modelo ya incluye las variables relevantes, entonces ya no habría nada que agregar La variabilidad muestral de \\(x_j\\): \\(\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2\\) Entre mayor sea la variabilidad menor es la varianza. Al aumentar el tamaño de muestra la variabilidad se incrementa y disminuye la varianza del estimador El grado de relación lineal entre las variables independientes: \\(R_j^2\\). Entre mayor sea la correlación la varianza es más grande. Una alta correlación siginifica que a pesar de tener muchos datos tengo poca información 2.3 Error estándar e inferencia Como \\(\\sigma^2\\) no es observable \\(Var(\\hat{\\beta_j}|\\mathbf{X})\\) no es computable. Para ello debo tener un estimador insesgado de \\(\\sigma^2\\), esto es un \\(\\hat{\\sigma}^2\\) tal que \\(E(\\hat{\\sigma}^2)=\\sigma^2\\), y por lo tanto que tengamos un estimador insesgado de la varianza del estimador Como \\(\\sigma^2=E(u^2)\\), entonces un estimador es la media muestral, promedio, de los residuales \\[\\begin{equation} \\hat{\\sigma}^2=\\dfrac{\\sum_{i=1}^n\\hat{u}_i^2}{n-k} \\end{equation}\\] Luego el error estándar es \\[\\begin{equation} \\tag{12} se(\\hat{\\beta})=\\dfrac{\\hat{\\sigma}}{\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2(1-R_j^2)} \\end{equation}\\] Importante. La formula anterior es válida bajo el supuesto de homocedasticidad El próximo paso es hacer inferencia estadística, es decir la realización de pruebas de hipótesis. Para ello necesitamos la distribución muestral de \\(\\hat{\\beta_j}\\). De la ecuación \\((9)\\) es claro que la distribución muestral, condicionada en las independientes, depende del error. S5 El error se distribuye normal con media cero y varianza \\(\\sigma^2\\): \\(u\\sim N(0,\\sigma^2)\\) Bajo los supuestos anteriores y S5, tenemos entonces que \\[\\begin{equation} \\tag{13} \\hat{\\beta}_j\\sim N(\\beta,Var(\\hat{\\beta})) \\end{equation}\\] Luego \\[\\begin{equation} \\tag{14} \\dfrac{\\hat{\\beta}_j-\\beta_j}{sd(\\hat{\\beta}_j)}\\sim N(0,1) \\end{equation}\\] Para hacer pruebas de hipótesis sobre un solo parámetro usamos \\((14)\\) pero teniendo en cuenta que \\(sd(\\hat{\\beta})\\) no es observable, pero su estimación es el error estándar, de donde tenemos que \\[\\begin{equation} \\tag{15} \\dfrac{\\hat{\\beta}_j-\\beta_j}{se(\\hat{\\beta}_j)}\\sim t_{n-k} \\end{equation}\\] Ahora, para probar \\(H_0:\\beta_j=0\\) usamos la estadística \\(t\\equiv(\\hat{\\beta_j}-\\beta_{j,H_0})/se(\\hat{\\beta}_j)\\). Esta me dice que tanto se desvia el valor estimado del valor bajo la hipótesis nula en relación a la desviación estándar. Por ejemplo, si \\(t=1\\) decimos que el valor estimado es mayor a cero en una desviación estándar del estimador. Dado que se obtiene un valor puntual de \\(\\hat{\\beta_j}\\), pero sabemos que pudimos haber obtenido un valor diferente con otra muestra, entonce debemos examinar la distribución de \\(\\hat{\\beta}_j\\) para saber que tan probable es que hubiésemos obtenido un valor estimado de cero. La prueba \\(t\\) me permite responder esa pregunta Demostración Vamos a simular unos datos \\(Y=\\alpha+\\beta_{X1}X_1+\\beta_{X2}X_2+u\\). Empezaremos simulando un proceso con \\(\\alpha=5\\), \\(\\beta_{X1}=0.5\\), \\(\\beta_{X2}=3\\). Tomamos inicialmente una muestra \\(n\\approx 100\\), y estimamos los coeficientes vía MCO Simulamos los datos y tomamos una muestra library(mvtnorm) library(ggplot2) N&lt;-10000 coefs&lt;-cbind(&quot;hat_beta_1&quot; = numeric(1000), &quot;hat_beta_2&quot; = numeric(1000)) #Vector que guardará los coeficientes set.seed(1) # permite reproducir los resultados X &lt;- rmvnorm(N, c(50, 100), sigma = cbind(c(10, 2.5), c(2.5, 10))) # generamos X1 y X2 u &lt;- rnorm(N, sd = 5) Y &lt;- 5 + 0.5 * X[, 1] + 3 * X[, 2] + u xdf&lt;-data.frame(X,Y) dfs&lt;-sample(c(TRUE,FALSE),nrow(xdf),replace=TRUE,prob=c(0.01,0.99)) #muestra aleatoria, n aprox 100 dfs&lt;-xdf[dfs,] Estimamos por MCO model1&lt;-lm(Y~X1+X2,data=dfs) model1 Call: lm(formula = Y ~ X1 + X2, data = dfs) Coefficients: (Intercept) X1 X2 -12.4896 0.9628 2.9362 Calculamos el error estándar y el estadístico \\(t\\). Tenga en cuenta que para estimar la varianza del estimador necesitamos \\[\\hat{\\sigma}^2=\\dfrac{\\sum_{i}^{n}\\hat{u}_i}{n-k}\\] X1bar&lt;-mean(dfs$X1) sumX1sqr&lt;-sum((dfs$X1-X1bar)^2) Rsqrx1&lt;-summary(lm(X1~X2,data=dfs))$r.squared uhat&lt;-model1$residuals sigmahat&lt;-(sum(uhat^2))/(model1$df.residual)#Varianza estimada del error varhatb1hat&lt;-sigmahat/(sumX1sqr*(1-Rsqrx1))#Varianza estimada del estimador se&lt;-sqrt(varhatb1hat) se [1] 0.1990679 t&lt;-coef(model1)[2]/se t X1 4.836502 Hacemos la prueba de hipótesis \\(Ho:\\beta_{X1}=0\\) contra la alternativa \\(Ha:\\beta_{X1}\\neq0\\). Grafiquemos la distribución \\(t_{df}\\) con los grados de libertar correspondientes y veamos donde se ubica nuestro estadístico \\(t\\) funcShaded &lt;- function(x) { y &lt;- dt(x,df=87) y[x &gt; -2&amp;x&lt;2 ] &lt;- NA return(y) } tdst&lt;- ggplot(data.frame(x = c(-4, 4)), aes(x = x)) + stat_function(fun = dt, args = list(df = model1$df.residual)) tdst+theme_classic()+stat_function(fun=funcShaded,geom=&quot;area&quot;, fill=&quot;blue&quot;,alpha=0.2)+ annotate(&quot;text&quot;, x =-3 , y = 0.1,label=&quot;Area=0.25&quot;)+annotate(&quot;text&quot;, x =3 , y = 0.1,label=&quot;Area=0.25&quot;)+ labs(title=&quot;Zonas de rechazo al 5%, df=60&quot;,y=&quot;&quot;,x=&quot;&quot;) Ejercicio Ingrese a la carpeta Data en el repositorio y descargue la base de datos wagew.rda. Esta base contiene los datos de la GEIH 2019. La base tiene información para personas ocupadas cuya posición ocupacional es empleado de empresa particular o empleado del gobierno. Contiene las siguientes variables p6020: 1 hombre, 2 mujer p6040: edad en años p6210: nivel educativo más alto alcanzado. 1 ninguno, 3 primaria, 4 básica (6-9), 5 media (10-13), 6 superior o universitaria p6210s1: último grado aprobado impa: ingreso monetario mensual sy: años de escolaridad Su propósito es estimar el retorno a la educación, en otras palabras, en cuánto aumenta el ingreso por cada año adicional de educación. Para ello plantea una ecuación de Mincer de la forma \\[ ln(w_i)=\\alpha+\\beta_1S_i+\\beta_2Exp_i+\\beta_3Exp^2+\\gamma Mujer_i+e_i \\] Donde \\(ln(w_i)\\): logaritmo del ingreso laboral mensual \\(S_i\\): años de escolaridad \\(Exp\\): es la experiencia potencial en el mercado laboral, la cual se calcula como \\(edad-S_i-6\\) \\(Mujer\\): dummy que identifica a las mujeres Antes de realizar la estimación, lleve a cabo el siguiente análisis gráfico Haga un gráfico de dispersión (scatter) donde tenga los años de escolaridad en el eje X y \\(ln(w)\\) en el eje Y (use ggplot con geom_point). Interprete En una gráfica superponga la distribución empírica, densidad, de \\(ln(w)\\) para hombres y para \\(mujeres\\). Interprete los resultados Realice la estimación del modelo y reporte los resultados en una tabla bien ordenada. Interprete los coeficientes en términos de dirección, magnitud y significancia2 ¿El coeficiente estimado \\(\\hat{\\beta_1}\\) tiene interpretación causal? Explique. Se sugiere revisar https://documents1.worldbank.org/curated/en/830831468147839247/pdf/WPS7020.pdf "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
